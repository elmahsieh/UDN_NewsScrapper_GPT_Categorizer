{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the UDN news GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import openai\n",
    "import jieba\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "from TCSP import read_stopwords_list\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandarallel.initial\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=5)\n",
    "\n",
    "tqdm.pandas(desc='GPT Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-4-32k-0314\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(content, model_name, max_response_tokens=1000):\n",
    "    \n",
    "    # Insert your personal API information\n",
    "    openai.api_type = [YOUR API TYPE]\n",
    "    openai.api_key = [YOUR API KEY]\n",
    "    openai.api_base = [YOUR API BASE]\n",
    "    openai.api_version = [YOUR API VERSION]\n",
    "\n",
    "    # Below is your prompt engineering\n",
    "    question = f'''\n",
    "                [INSERT YOUR OWN PROMPT ENGINEERING]\n",
    "                '''\n",
    "            \n",
    "    prompt = f\"你是一位專業的NLP工程師,請你參照表格內容請你幫我執行NER任務，表格：{content}，根據{question}處理\"\n",
    "\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\":question }\n",
    "            ]\n",
    "\n",
    "    print(num_tokens_from_messages(messages))\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "  \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c2d(title,herf,full,date,tag):\n",
    "    df = pd.DataFrame({'title': [title], 'href': [f'<a href=\"{herf}\">{herf}</a>'], 'full': [full], 'Date': [date], 'tag': [tag]})\n",
    "    md = df.to_markdown()\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"gpt4-32k\"\n",
    "EMBED_MODEL_NAME = \"ada002\"\n",
    "MAX_RESPONSE_TOKENS = 10000\n",
    "overall_max_tokens = 32000\n",
    "prompt_max_token = overall_max_tokens - MAX_RESPONSE_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_generated_info(response_message):\n",
    "    extracted_info = {}\n",
    "    \n",
    "    # Extracting 罪名\n",
    "    match_crime = re.search(r\"罪名: (.*?)\\n\", response_message)\n",
    "    if match_crime:\n",
    "        extracted_info[\"罪名\"] = match_crime.group(1)     \n",
    "    # Extracting 觸犯法規\n",
    "    match_law = re.search(r\"觸犯法規: (.*?)\\n\", response_message)\n",
    "    if match_law:\n",
    "        extracted_info[\"觸犯法規\"] = match_law.group(1)   \n",
    "    # Extracting 姓名\n",
    "    match_name = re.search(r\"姓名: (.*?)\\n\", response_message)\n",
    "    if match_name:\n",
    "        extracted_info[\"姓名\"] = match_name.group(1)   \n",
    "    # Extracting 年齡\n",
    "    match_age = re.search(r\"年齡: (.*?)\\n\", response_message)\n",
    "    if match_age:\n",
    "        extracted_info[\"年齡\"] = match_age.group(1) \n",
    "    # Extracting 頭銜\n",
    "    match_title = re.search(r\"頭銜: (.*?)\\n\", response_message)\n",
    "    if match_title:\n",
    "        extracted_info[\"頭銜\"] = match_title.group(1)   \n",
    "    # Extracting 職/產業\n",
    "    match_profession = re.search(r\"職/產業: (.*?)\\n\", response_message)\n",
    "    if match_profession:\n",
    "        extracted_info[\"職/產業\"] = match_profession.group(1)   \n",
    "    # Extracting 新聞報導地點\n",
    "    match_newslocation = re.search(r\"新聞報導地點: (.*?)\\n\", response_message)\n",
    "    if match_newslocation:\n",
    "        extracted_info[\"新聞報導地點\"] = match_newslocation.group(1) \n",
    "    # Extracting 發生地點\n",
    "    match_location = re.search(r\"發生地點: (.*?)\\n\", response_message)\n",
    "    if match_location:\n",
    "        extracted_info[\"發生地點\"] = match_location.group(1) \n",
    "    # Extracting 內文摘要\n",
    "    match_summary = re.search(r\"內文摘要: (.*?)\\n\", response_message)\n",
    "    if match_summary:\n",
    "        extracted_info[\"內文摘要\"] = match_summary.group(1) \n",
    "\n",
    "    print(response_message)\n",
    "    print(extracted_info)    \n",
    "\n",
    "    return extracted_info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(content):\n",
    "    response = send_message(content['md'], CHAT_MODEL_NAME, MAX_RESPONSE_TOKENS)\n",
    "    response_message = response['choices'][0]['message']['content']\n",
    "    generated_info = extract_generated_info(response_message)\n",
    "\n",
    "    return generated_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('udn_news.csv')\n",
    "content = res[['title','href','content',\"Date\",'tag']][15:20]\n",
    "#creates new column in datafram called 'md'\n",
    "content['md'] = content.apply(lambda x: c2d(x['title'],x['href'],x['content'],x['Date'],x['tag']),axis=1)\n",
    "\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"GPT總結{len(content)}篇新聞所需時間：{execution_time:.6f} 秒\")\n",
    "\n",
    "# Insert the provided code block to extract generated_info\n",
    "content['generated_info'] = content.apply(generate_content, axis=1)\n",
    "for col in tqdm(content['generated_info'].iloc[0].keys()):\n",
    "    content[col] = content['generated_info'].apply(lambda x: x[col])\n",
    "content.drop(['generated_info'], axis=1, inplace=True)\n",
    "\n",
    "content.drop(['md'],axis=1,inplace=True)\n",
    "\n",
    "content.to_csv('udn_news_Categorized.csv',index=False, encoding=\"utf-8_sig\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
